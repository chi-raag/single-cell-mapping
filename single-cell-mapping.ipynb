{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0035b1ff954923a2c4536941098c06c42e6a640eb7c410f31fb34346d4e9139b4",
   "display_name": "Python 3.9.2 64-bit ('single-cell-mapper': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Single Cell Mapper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Libraries\n",
    "\n",
    "Run the chunk below to import the required libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import *\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "from scipy import * \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "source": [
    "## Single Cell Mapper Autoencoder\n",
    "\n",
    "Run the chunk below to create the autoencoder class that will be trained later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_seq_ae:\n",
    "    \n",
    "    def __init__(self, ref_file, query_file):\n",
    "        self.ref_ann, self.query_ann = data_import_sc(ref_file, query_file)\n",
    "        self.ref_data = self.ref_ann.X\n",
    "        self.query_data = self.query_ann.X\n",
    "        self.ref_data_t = np.transpose(self.ref_data)\n",
    "        self.query_data_t = np.transpose(self.query_data)\n",
    "\n",
    "    def processed_data(self):\n",
    "        return self.ref_ann, self.query_ann\n",
    "\n",
    "    def processed_data_t(self):\n",
    "        return self.ref_ann_t, self.query_ann_t\n",
    "        \n",
    "    class Autoencoder(nn.Module):\n",
    "        \n",
    "        def __init__(self, ref_len, query_len, r_hd, q_hd, r_bd, q_bd, c_hd, c_bd):\n",
    "    \n",
    "            super(RNA_seq_ae.Autoencoder, self).__init__()\n",
    "\n",
    "            self.ref_encoder = nn.Sequential(\n",
    "                nn.Linear(ref_len, r_hd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(r_hd, r_bd))\n",
    "\n",
    "            self.query_encoder = nn.Sequential(\n",
    "                nn.Linear(query_len, q_hd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(q_hd, q_bd))\n",
    "\n",
    "            self.combined_encoder = nn.Sequential(\n",
    "                nn.Linear(q_bd + r_bd, c_hd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(c_hd, c_bd))\n",
    "            \n",
    "            self.ref_decoder = nn.Sequential(\n",
    "                nn.Linear(c_bd, r_hd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(r_hd, ref_len),\n",
    "                nn.Tanhshrink())\n",
    "\n",
    "            self.query_decoder = nn.Sequential(\n",
    "                nn.Linear(c_bd, q_hd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(q_hd, query_len),\n",
    "                nn.Tanhshrink())\n",
    "\n",
    "            \n",
    "        def forward(self, ref_batch, query_batch):\n",
    "            ref_batch = self.ref_encoder(ref_batch)\n",
    "            query_batch = self.query_encoder(query_batch)\n",
    "            combined_batch = torch.cat((ref_batch, query_batch), 1)\n",
    "            combined_batch = self.combined_encoder(combined_batch)\n",
    "            ref_batch = self.ref_decoder(combined_batch)\n",
    "            query_batch = self.query_decoder(combined_batch)\n",
    "            return ref_batch, query_batch\n",
    "        \n",
    "\n",
    "    def train_test(self, r_hd, q_hd, r_bd, q_bd, c_hd, c_bd, batch_size, n_epochs, lr):\n",
    "        \n",
    "        ref_len = len(np.ravel(self.ref_data_t[0]))\n",
    "        query_len = len(np.ravel(self.query_data_t[0]))\n",
    "        ref_batches, query_batches = batchify_autoencoder(self.ref_data_t, self.query_data_t, batch_size)\n",
    "        neural_network = RNA_seq_ae.Autoencoder(ref_len, query_len, r_hd, q_hd, r_bd, q_bd, c_hd, c_bd)\n",
    "        optimizer = optim.Adagrad(neural_network.parameters(), lr=lr)\n",
    "        loss_function = nn.MSELoss()\n",
    "        neural_network.train()\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            error = 0\n",
    "            for ii in range(len(ref_batches)):\n",
    "                optimizer.zero_grad()\n",
    "                ref_batch = ref_batches[ii]\n",
    "                query_batch = query_batches[ii]\n",
    "                predictions_ref, predictions_query = neural_network(torch.tensor(np.asarray(ref_batch).astype(np.float32)), torch.tensor(np.asarray(query_batch).astype(np.float32)))\n",
    "                loss_ref = loss_function(predictions_ref, torch.tensor(np.asarray(ref_batch).astype(np.float32)))\n",
    "                loss_query = loss_function(predictions_query, torch.tensor(np.asarray(query_batch).astype(np.float32)))\n",
    "                loss = loss_ref + loss_query                   \n",
    "                loss.backward()      \n",
    "                optimizer.step()\n",
    "                error += loss.data\n",
    "\n",
    "            if i%1000 == 0:\n",
    "                print(\"Epoch\", i, \": Loss =\", error) \n",
    "            \n",
    "        return neural_network"
   ]
  },
  {
   "source": [
    "## Helper Functions\n",
    "\n",
    "Run the chunk below to create helper functions that will assist with data importation, batch creation, embedding generation, and the visualization of results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import_sc(ref_sc, query_sc):\n",
    "\n",
    "        ref_data = ref_sc\n",
    "        query_data = query_sc\n",
    "\n",
    "        sc.pp.filter_cells(ref_data, min_genes = 200)\n",
    "        sc.pp.filter_genes(ref_data, min_cells = 3)\n",
    "\n",
    "        sc.pp.filter_cells(query_data, min_genes = 200)\n",
    "        sc.pp.filter_genes(query_data, min_cells = 3)\n",
    "\n",
    "        ref_len = len(ref_data.obs)\n",
    "        query_len = len(query_data.obs)\n",
    "\n",
    "        var_names = ref_data.var_names.intersection(query_data.var_names)\n",
    "        ref_data = ref_data[:, var_names]\n",
    "        query_data = query_data[:, var_names]\n",
    "\n",
    "        return ref_data, query_data\n",
    "    \n",
    "    \n",
    "def batchify_autoencoder(ref, query, batch_size=16):\n",
    "    \n",
    "    query_batches = []\n",
    "    ref_batches = []\n",
    "    for n in range(0, len(query),batch_size):\n",
    "        if n+batch_size < len(query):\n",
    "            query_batch = query[n:n+batch_size]\n",
    "            ref_batch = ref[n:n+batch_size]\n",
    "            query_batches.append(query_batch)\n",
    "            ref_batches.append(ref_batch)\n",
    "            \n",
    "    if len(query)%batch_size > 0:\n",
    "        query_batch = query[len(query)-(len(query)%batch_size):len(query)]\n",
    "        ref_batch = ref[len(ref)-(len(ref)%batch_size):len(ref)]\n",
    "        query_batches.append(query_batch)\n",
    "        ref_batches.append(ref_batch)\n",
    "  \n",
    "    return ref_batches, query_batches\n",
    "\n",
    "\n",
    "def create_embed_df(matrix):\n",
    "    df = pd.DataFrame()\n",
    "    for col in range(matrix.shape[1]):\n",
    "        df[\"em_\" + str(col+1)] = matrix[:,col].detach().numpy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_embeddings(model, ref_ann, query_ann, n_pca):\n",
    "    ref = torch.from_numpy(ref_ann.X)\n",
    "    query = torch.from_numpy(query_ann.X)\n",
    "    ref_t = np.transpose(torch.from_numpy(ref_ann.X))\n",
    "    query_t = np.transpose(torch.from_numpy(query_ann.X))\n",
    "    ref_em = model.ref_encoder(ref_t)\n",
    "    query_em = model.query_encoder(query_t)\n",
    "    com = torch.cat((ref_em, query_em), 1)\n",
    "    com = model.combined_encoder(com)\n",
    "    pca_model = PCA(n_components=n_pca)\n",
    "    ref_pca = pca_model.fit_transform(ref)\n",
    "    query_pca = pca_model.fit_transform(query)\n",
    "    com_df = create_embed_df(com)\n",
    "\n",
    "    query_embeds = pd.DataFrame(np.dot(query_pca, np.transpose(com_df)))\n",
    "    ref_embeds = pd.DataFrame(np.dot(ref_pca, np.transpose(com_df)))\n",
    "    return ref_embeds, query_embeds"
   ]
  },
  {
   "source": [
    "## Model Initialization\n",
    "\n",
    "Run the chunk below to initialize the model with the specified data sets, and to return preprocessed data that will be used later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ref = sc.datasets.pbmc3k_processed()\n",
    "adata = sc.datasets.pbmc68k_reduced()\n",
    "sc_map_test = RNA_seq_ae(adata_ref, adata)\n",
    "ref_ann, query_ann = sc_map_test.processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 : Loss = tensor(38.4837)\n",
      "Epoch 1000 : Loss = tensor(8.1753)\n",
      "Epoch 2000 : Loss = tensor(5.6159)\n",
      "Finished model\n",
      "Epoch 0 : Loss = tensor(45.4452)\n",
      "Epoch 1000 : Loss = tensor(4.9985)\n",
      "Epoch 2000 : Loss = tensor(2.7435)\n",
      "Finished model\n",
      "Epoch 0 : Loss = tensor(45.4615)\n",
      "Epoch 1000 : Loss = tensor(3.4567)\n",
      "Epoch 2000 : Loss = tensor(1.9989)\n",
      "Finished model\n",
      "Epoch 0 : Loss = tensor(32.3752)\n",
      "Epoch 1000 : Loss = tensor(2.4176)\n",
      "Epoch 2000 : Loss = tensor(1.3412)\n",
      "Finished model\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "r_hd = [1000, 1000, 1000, 1000]\n",
    "q_hd = [600, 600, 600, 600]\n",
    "r_bd = [200, 200, 200, 200]\n",
    "q_bd = [100, 100, 100, 100]\n",
    "c_hd = [100, 100, 100, 100]\n",
    "c_bd = [3, 5, 10, 20]\n",
    "for p1, p2, p3, p4, p5, p6 in zip(r_hd, q_hd, r_bd, q_bd, c_hd, c_bd):\n",
    "    m = sc_map_test.train_test(r_hd=p1, q_hd=p2, r_bd=p3, q_bd=p4, c_hd=p5, c_bd=p6, batch_size = 16, n_epochs = 3000, lr=.01)\n",
    "    models.append(m)\n",
    "    print(\"Finished model\")"
   ]
  },
  {
   "source": [
    "## Results\n",
    "\n",
    "Run the chunk below to produce the results of nearest neighbors clustering on the corrected cell count matrix, and the one below that for the results on the original cell count matrix. While multiple models were trained, only one's results are demonstrated below. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.25675675675675674"
      ]
     },
     "metadata": {},
     "execution_count": 325
    }
   ],
   "source": [
    "adata_ref = sc.datasets.pbmc3k_processed()\n",
    "adata = sc.datasets.pbmc68k_reduced()\n",
    "sc_map_test = RNA_seq_ae(adata_ref, adata)\n",
    "ref_ann, query_ann = sc_map_test.processed_data()\n",
    "\n",
    "# create ground truth labels for query cells, and merge similar cell types\n",
    "ref_y = list(ref_ann.obs['louvain'])\n",
    "for i in range(0, len(ref_y)):\n",
    "    if ref_y[i] in ['CD14+ Monocytes', 'FCGR3A+ Monocytes']:\n",
    "        ref_y[i] = 'Monocytes'\n",
    "\n",
    "ref_embeds, query_embeds = create_embeddings(models[1], ref_ann, query_ann, 5)\n",
    "ref_ann.X = ref_embeds\n",
    "ref_ann.obs['louvain'] = ref_y\n",
    "query_ann.X = query_embeds\n",
    "sc.pp.pca(ref_ann)\n",
    "sc.pp.neighbors(ref_ann)\n",
    "sc.tl.umap(ref_ann)\n",
    "sc.tl.ingest(query_ann, ref_ann, obs='louvain')\n",
    "\n",
    "query_y = list(query_ann.obs['bulk_labels'])\n",
    "for i in range(0, len(query_y)):\n",
    "    if query_y[i] in ['CD4+/CD25 T Reg', 'CD4+/CD45RA+/CD25- Naive T', 'CD4+/CD45RO+ Memory']:\n",
    "        query_y[i] = 'CD4 T cells'\n",
    "    elif query_y[i] in ['CD8+ Cytotoxic T', 'CD8+/CD45RA+ Naive Cytotoxic']:\n",
    "        query_y[i] = 'CD8 T cells'\n",
    "    elif query_y[i] in ['CD14+ Monocytes']:\n",
    "        query_y[i] = 'Monocytes'\n",
    "    elif query_y[i] in ['CD19+ B']:\n",
    "        query_y[i] = 'B cells'\n",
    "    elif query_y[i] == 'Dendritic':\n",
    "        query_y[i] = 'Dendritic cells'\n",
    "    elif query_y[i] in ['CD56+ NK']:\n",
    "        query_y[i] = \"NK cells\"\n",
    "\n",
    "query_ann.obs['bulk_labels'] = query_y\n",
    "accuracy_score(list(query_ann.obs['bulk_labels']), list(query_ann.obs['louvain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.22635135135135134"
      ]
     },
     "metadata": {},
     "execution_count": 326
    }
   ],
   "source": [
    "adata_ref = sc.datasets.pbmc3k_processed()\n",
    "adata = sc.datasets.pbmc68k_reduced()\n",
    "sc_map_test = RNA_seq_ae(adata_ref, adata)\n",
    "ref_ann, query_ann = sc_map_test.processed_data()\n",
    "\n",
    "ref_y = list(ref_ann.obs['louvain'])\n",
    "for i in range(0, len(ref_y)):\n",
    "    if ref_y[i] in ['CD14+ Monocytes', 'FCGR3A+ Monocytes']:\n",
    "        ref_y[i] = 'Monocytes'\n",
    "\n",
    "ref_ann.obs['louvain'] = ref_y\n",
    "sc.pp.pca(ref_ann)\n",
    "sc.pp.neighbors(ref_ann)\n",
    "sc.tl.umap(ref_ann)\n",
    "sc.tl.ingest(query_ann, ref_ann, obs='louvain')\n",
    "\n",
    "query_y = list(query_ann.obs['bulk_labels'])\n",
    "for i in range(0, len(query_y)):\n",
    "    if query_y[i] in ['CD4+/CD25 T Reg', 'CD4+/CD45RA+/CD25- Naive T', 'CD4+/CD45RO+ Memory']:\n",
    "        query_y[i] = 'CD4 T cells'\n",
    "    elif query_y[i] in ['CD8+ Cytotoxic T', 'CD8+/CD45RA+ Naive Cytotoxic']:\n",
    "        query_y[i] = 'CD8 T cells'\n",
    "    elif query_y[i] in ['CD14+ Monocytes']:\n",
    "        query_y[i] = 'Monocytes'\n",
    "    elif query_y[i] in ['CD19+ B']:\n",
    "        query_y[i] = 'B cells'\n",
    "    elif query_y[i] == 'Dendritic':\n",
    "        query_y[i] = 'Dendritic cells'\n",
    "    elif query_y[i] in ['CD56+ NK']:\n",
    "        query_y[i] = \"NK cells\"\n",
    "\n",
    "query_ann.obs['bulk_labels'] = query_y\n",
    "accuracy_score(list(query_ann.obs['bulk_labels']), list(query_ann.obs['louvain']))"
   ]
  }
 ]
}